{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13c1dd7",
   "metadata": {},
   "source": [
    "# Running a Multi-Step Workflow with a Remote System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81fa1b",
   "metadata": {},
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ecbb39",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to connect and execute different types of tasks within a single workflow, including running parts of the workflow on an external computer, or tasks which require dynamic inputs. As an example, we begin with an initial structure, perform `geometry optimization`, compute `descriptors`, and run a script to split the structures into training files. We then run `Quantum Espresso` on an external system (SCARF) to calculate energies, and finally use the output files to train a model. The goal is to illustrate how to design and execute workflows that seamlessly integrate local and remote tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf0b46",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "For this tutorial we will assume you have: \n",
    "<ul>\n",
    "        <li>A AiiDA profile setup</li>\n",
    "        <li>An external computer setup in AiiDA with a quantum espresso code</li>\n",
    "                <ul>\n",
    "                        <li>A tutorial can be found in <code>../aiida_setup/setup-external-computer.ipynb</code></li>\n",
    "                </ul>\n",
    "        <li>The <code>aiida-quantumespresso</code>, <code>aiida-pseudo</code> and <code>fpsample</code> extra dependancies installed</li>\n",
    "        <li>Pseudopotentails SSSP installed</li>\n",
    "                <ul>\n",
    "                        <li>They can be installed with: <code>aiida-pseudo install sssp</code></li>\n",
    "                </ul>\n",
    "\n",
    "</ul>\n",
    "\n",
    "The initial setup is very similar to the other tutorials, such as `singlepoint.ipynb`, which goes into more detail about what each step is doing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfb65e",
   "metadata": {},
   "source": [
    "Load the aiida profile, model and the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79098139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida import load_profile\n",
    "load_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida_mlip.data.model import ModelData\n",
    "uri = \"https://github.com/stfc/janus-core/raw/main/tests/models/mace_mp_small.model\"\n",
    "model = ModelData.from_uri(uri, architecture=\"mace_mp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b9564",
   "metadata": {},
   "source": [
    "This time we also load the code which executes the process on the external computer, `qe@scarf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida.orm import load_code\n",
    "\n",
    "janus_code = load_code(\"janus@localhost\")\n",
    "qe_code = load_code(\"qe@scarf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d661593",
   "metadata": {},
   "source": [
    "We must now choose the calculations to perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4c78c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida.plugins import CalculationFactory\n",
    "geomoptCalc = CalculationFactory(\"mlip.opt\")\n",
    "descriptorsCalc = CalculationFactory(\"mlip.descriptors\")\n",
    "trainCalc = CalculationFactory(\"mlip.train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df1f81",
   "metadata": {},
   "source": [
    "Before setting up the work graph, we first configure the `Quantum Espresso (QE)` task by defining the code and input parameters. Since we need to run QE on multiple structures, we create multiple `PwCalculation` tasks dynamically within the same task using `get_current_graph()`. This allows us to run QE for each structure and return the corresponding `TrajectoryData` and parameters for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida_workgraph import task\n",
    "from aiida_workgraph.manager import get_current_graph\n",
    "from aiida.orm import StructureData, load_group, KpointsData, SinglefileData\n",
    "from ase.io import iread\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from aiida_quantumespresso.calculations.pw import PwCalculation\n",
    "from sample_split import process_and_split_data\n",
    "\n",
    "\n",
    "@task.graph(outputs = [\"test_file\", \"train_file\", \"valid_file\"])\n",
    "def qe(**inputs):\n",
    "\n",
    "    wg = get_current_graph()\n",
    "\n",
    "    task_inputs = inputs[\"task_params\"]['task_inputs']\n",
    "    code =inputs[\"task_params\"][\"code\"]\n",
    "\n",
    "    kpoints = KpointsData()\n",
    "    kpoints.set_kpoints_mesh(task_inputs['kpoint_mesh'])\n",
    "\n",
    "    pseudo_family = load_group('SSSP/1.3/PBE/efficiency')\n",
    "    files = {\"test_file\": inputs['test_file'],\"train_file\":inputs['train_file'],\"valid_file\":inputs['valid_file']}\n",
    "\n",
    "    for file_name, file in files.items():\n",
    "        with file.as_path() as path:\n",
    "            for i, structs in enumerate(iread(path, format=\"extxyz\")):\n",
    "                \n",
    "                structure = StructureData(ase=structs)\n",
    "                pseudos = pseudo_family.get_pseudos(structure=structure)\n",
    "\n",
    "                ecutwfc, ecutrho = pseudo_family.get_recommended_cutoffs(\n",
    "                    structure=structure,\n",
    "                    unit='Ry',\n",
    "                )\n",
    "\n",
    "                pw_params = {\n",
    "                    \"CONTROL\": {\n",
    "                        \"calculation\": \"scf\",\n",
    "                        'tprnfor': True,\n",
    "                        'tstress': True,\n",
    "                    },\n",
    "                    \"SYSTEM\": {\n",
    "                        \"ecutwfc\": ecutwfc,\n",
    "                        \"ecutrho\": ecutrho,\n",
    "                    },\n",
    "                }\n",
    "                \n",
    "                qe_task = wg.add_task(\n",
    "                    PwCalculation,\n",
    "                    code = code,\n",
    "                    parameters= pw_params,\n",
    "                    kpoints= kpoints,\n",
    "                    pseudos= pseudos,\n",
    "                    metadata= task_inputs[\"metadata\"],\n",
    "                    structure= structure,\n",
    "                )\n",
    "                \n",
    "                structfile = f\"{file_name}.struct{i}\"\n",
    "\n",
    "                wg.update_ctx({\n",
    "                    structfile:{\n",
    "                        \"trajectory\":qe_task.outputs.output_trajectory,\n",
    "                        \"parameters\": qe_task.outputs.output_parameters\n",
    "                    }\n",
    "                })\n",
    "\n",
    "    return {\n",
    "        \"test_file\": wg.ctx.test_file,\n",
    "        \"train_file\": wg.ctx.train_file,\n",
    "        \"valid_file\": wg.ctx.valid_file\n",
    "    }    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6fb5b0",
   "metadata": {},
   "source": [
    "The next task we need is a function which can extract the required parameters from the QE tasks and create the files for training. This task creates `mlip_[file]_file.extxyz` and returns a `JanusConfigfile` which is used for the training calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida_mlip.data.config import JanusConfigfile\n",
    "from aiida.orm import Dict\n",
    "from ase.io import write\n",
    "from ase import units\n",
    "\n",
    "@task.calcfunction(outputs = [\"JanusConfigfile\"])\n",
    "def create_train_file(**inputs):\n",
    "\n",
    "    training_files = {}\n",
    "    \n",
    "    for file_name, structs in inputs.items():\n",
    "        path = Path(f\"mlip_{file_name}.extxyz\")\n",
    "\n",
    "        for stuct_out_params in structs.values():\n",
    "            \n",
    "            trajectory = stuct_out_params[\"trajectory\"]\n",
    "\n",
    "            fileStructure = trajectory.get_structure(index=0)\n",
    "            fileAtoms = fileStructure.get_ase()\n",
    "\n",
    "            stress = trajectory.arrays[\"stress\"][0]\n",
    "            converted_stress = stress * units.GPa\n",
    "            fileAtoms.info[\"qe_stress\"] = converted_stress\n",
    "\n",
    "            fileAtoms.info[\"units\"] = {\"energy\": \"eV\",\"forces\": \"ev/Ang\",\"stress\": \"ev/Ang^3\"}\n",
    "            fileAtoms.set_array(\"qe_forces\", trajectory.arrays[\"forces\"][0])\n",
    "\n",
    "            parameters = stuct_out_params[\"parameters\"]\n",
    "            fileParams = parameters.get_dict()\n",
    "            fileAtoms.info[\"qe_energy\"] = fileParams[\"energy\"]\n",
    "            write(path, fileAtoms, append=True)\n",
    "\n",
    "        training_files[file_name] = str(path.resolve())\n",
    "\n",
    "    with open(\"JanusConfigfile.yml\", \"a\") as f:\n",
    "        yaml.safe_dump(training_files, f, sort_keys=False)\n",
    "\n",
    "    return{'JanusConfigfile': JanusConfigfile(Path(\"JanusConfigfile.yml\").resolve())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8534d73f",
   "metadata": {},
   "source": [
    "For this task, we are using a task to run a pure python function. This is to demonstrate the flexibility of tasks and how you can run python functions on the workchain. This returns `SinglefileData` instances of the test, train and valid files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task.calcfunction(outputs = [\"test_file\", \"train_file\", \"valid_file\"])\n",
    "def create_aiida_files(**inputs):\n",
    "     \n",
    "    files = process_and_split_data(**inputs)\n",
    "\n",
    "    return {\n",
    "        \"train_file\": SinglefileData(files[\"train_file\"]),\n",
    "        \"test_file\": SinglefileData(files[\"test_file\"]),\n",
    "        \"valid_file\": SinglefileData(files[\"valid_file\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aebeed5",
   "metadata": {},
   "source": [
    "We initialize the inputs we want for all the calculations. These variables can be changed depending on the configuration you are running and whether you want to change any inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5760b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida.orm import Str, Float, Bool, Int, List\n",
    "\n",
    "calc_inputs = {\n",
    "    \"code\": janus_code,\n",
    "    \"model\": model,\n",
    "    \"arch\": Str(model.architecture),\n",
    "    \"device\": Str(\"cpu\"),\n",
    "    \"metadata\": {\"options\": {\"resources\": {\"num_machines\": 1}}},\n",
    "}\n",
    "\n",
    "goemopt_inputs = {\n",
    "    \"fmax\": Float(0.1),\n",
    "    \"opt_cell_lengths\": Bool(False),\n",
    "    \"opt_cell_fully\": Bool(True),\n",
    "}\n",
    "\n",
    "split_task_inputs = {\n",
    "    \"config_types\": Str(\"\"),\n",
    "    \"prefix\": Str(\"\"),\n",
    "    \"scale\": Float(1.0e5),\n",
    "    \"append_mode\": Bool(False),\n",
    "}\n",
    "\n",
    "qe_inputs = {\n",
    "    \"task_inputs\": Dict({\n",
    "        \"metadata\": {\n",
    "            \"options\": {\n",
    "                \"resources\": {\n",
    "                    \"num_machines\": 1,\n",
    "                    \"num_mpiprocs_per_machine\": 32,\n",
    "                },\n",
    "                \"max_wallclock_seconds\": 3600,\n",
    "                \"queue_name\": \"scarf\",\n",
    "                \"qos\": \"scarf\",\n",
    "                \"environment_variables\": {},\n",
    "                \"withmpi\": True,\n",
    "                \"prepend_text\": \"\"\"\n",
    "                    module purge\n",
    "                    module use /work4/scd/scarf562/eb-common/modules/all\n",
    "                    module load amd-modules\n",
    "                    module load QuantumESPRESSO/7.2-foss-2023a\n",
    "                \"\"\",\n",
    "                \"append_text\": \"\",\n",
    "            },\n",
    "        },\n",
    "        \"kpoint_mesh\": List([1, 1, 1]),\n",
    "    }),\n",
    "    \"code\": qe_code,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee80fd",
   "metadata": {},
   "source": [
    "Now we can build the `Workgraph`. First we iterate through each structure in the initail structure file, and run `Geomopt` and `Descriptors` on them these give a `SinglefileData` instance of the structure outputs. These structures can then be passed to the `split_task`, which splits these structures up into training files. Then we run `QE` task, getting the outputs and passing them into the `training_files` task which, as the name suggests, it creates the training file from the `QE` task outputs. Finally we can run the training script. Ideally, if any of the inputs need to changed, they should be done in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0299e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida_workgraph import WorkGraph\n",
    "from aiida.orm import StructureData\n",
    "from ase.io import iread\n",
    "\n",
    "initial_structure = \"../structures/NaCl-traj.xyz\"\n",
    "\n",
    "with WorkGraph(\"QE Calculation Workgraph\") as wg:\n",
    "\n",
    "    final_structures = {}\n",
    "\n",
    "    for i, struct in enumerate(iread(initial_structure)):\n",
    "        structure = StructureData(ase=struct)\n",
    "        \n",
    "        geomopt_calc = wg.add_task(\n",
    "            geomoptCalc,\n",
    "            **calc_inputs,\n",
    "            **goemopt_inputs,\n",
    "            struct=structure,\n",
    "        )\n",
    "        \n",
    "        descriptors_calc = wg.add_task(\n",
    "            descriptorsCalc,\n",
    "            **calc_inputs,\n",
    "            struct=geomopt_calc.outputs.final_structure,\n",
    "            calc_per_element=True,\n",
    "        )\n",
    "\n",
    "        final_structures[f\"structs{i}\"] = descriptors_calc.outputs.xyz_output\n",
    "\n",
    "    split_task = wg.add_task(\n",
    "        create_aiida_files, \n",
    "        **split_task_inputs,\n",
    "        trajectory_data=final_structures,\n",
    "        n_samples= Int(len(final_structures)),\n",
    "        )\n",
    "    \n",
    "    qe_task = wg.add_task(\n",
    "        qe, \n",
    "        name=\"QE_workflow\",\n",
    "        test_file= split_task.outputs.test_file,\n",
    "        train_file= split_task.outputs.train_file,\n",
    "        valid_file= split_task.outputs.valid_file,\n",
    "        task_params = qe_inputs\n",
    "    )\n",
    "\n",
    "    training_files = wg.add_task(\n",
    "        create_train_file, \n",
    "        test_file= qe_task.outputs.test_file,\n",
    "        train_file= qe_task.outputs.train_file,\n",
    "        valid_file= qe_task.outputs.valid_file,\n",
    "        )\n",
    "\n",
    "    train_task = wg.add_task(\n",
    "        trainCalc,\n",
    "        mlip_config=training_files.outputs.JanusConfigfile,\n",
    "        code=calc_inputs[\"code\"],\n",
    "        foundation_model=calc_inputs[\"model\"],\n",
    "        metadata=calc_inputs[\"metadata\"],\n",
    "        fine_tune=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c72ca",
   "metadata": {},
   "source": [
    "Run and visualise the workgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5fd6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36396f",
   "metadata": {},
   "source": [
    "If we want to get the training plot, we have to pull it from the remote folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9cd027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "folder = wg.tasks.Train.outputs.remote_folder.value\n",
    "picturePath = f\"{os.getcwd()}/traingraph.png\"\n",
    "folder.getfile(relpath='results/test_run-123_train_Default_stage_one.png',destpath=picturePath)\n",
    "\n",
    "img = mpimg.imread(picturePath)\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiida-mlip (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
