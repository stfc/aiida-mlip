{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ecbb39",
   "metadata": {},
   "source": [
    "# WorkGraph Tutorial: MD Simulation with DFT Labeling and MLIP Fine-tuning\n",
    "\n",
    "This tutorial demonstrates building complex workflows with AiiDA WorkGraph making use of a remote system (SCARF), and using Quantum ESPRESSO for generating DFT-labeled training data and fine-tuning a machine learning interatomic potential (MLIP).\n",
    "\n",
    "## Workflow Steps\n",
    "\n",
    "1. **MD Simulation** → Generate diverse NaCl configurations (6 snapshots)\n",
    "2. **Descriptors** → Calculate MLIP features for filtering (dynamic tasks)\n",
    "3. **Data Split** → FPS filtering into train/validation/test sets\n",
    "4. **DFT Labeling** → Quantum ESPRESSO energies, forces, stresses (nested dynamic tasks)\n",
    "5. **Training Files** → Format ExtXYZ files with DFT labels\n",
    "6. **Fine-tuning** → Train MACE model on DFT data\n",
    "\n",
    "## WorkGraph Features\n",
    "\n",
    "- **Dynamic task generation**: Create tasks at runtime based on trajectory length\n",
    "- **Nested namespaces**: Organize complex hierarchical outputs\n",
    "- **Automatic dependencies**: Tasks execute when inputs become available\n",
    "- **Parallel execution**: Independent calculations run simultaneously\n",
    "- **Provenance tracking**: Full AiiDA database tracking of all calculations and data for reproducibility\n",
    "- **Mixed local and remote execution**: MD runs locally while DFT calculations execute on HPC clusters\n",
    "\n",
    "### Setup\n",
    "\n",
    "For this tutorial we will assume you have: \n",
    "<ul>\n",
    "        <li>An AiiDA profile setup</li>\n",
    "        <li>An external computer setup in AiiDA with a quantum espresso code</li>\n",
    "                <ul>\n",
    "                        <li>A tutorial can be found in <a href=\"../aiida_setup/setup-external-computer.ipynb\"><code>../aiida_setup/setup-external-computer.ipynb</code></a></li>\n",
    "                </ul>\n",
    "        <li>The <code>aiida-quantumespresso</code>, <code>aiida-pseudo</code> and <code>fpsample</code> extra dependancies installed</li>\n",
    "        <li>Pseudopotentails SSSP installed</li>\n",
    "                <ul>\n",
    "                        <li>They can be installed with: <code>aiida-pseudo install sssp</code></li>\n",
    "                </ul>\n",
    "\n",
    "</ul>\n",
    "\n",
    "The initial setup is very similar to the other tutorials, such as [singlepoint.ipynb](https://stfc.github.io/aiida-mlip/user_guide/notebooks/singlepoint.html), which goes into more detail about what each step is doing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfb65e",
   "metadata": {},
   "source": [
    "### Load AiiDA Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79098139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida import load_profile\n",
    "load_profile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d393384",
   "metadata": {},
   "source": [
    "### Load ML Potential Model\n",
    "\n",
    "We download a pre-trained MACE model from the janus-core repository. \n",
    "\n",
    "The `ModelData.from_uri()` function automatically caches the model locally to avoid repeated downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida_mlip.data.model import ModelData\n",
    "uri = \"https://github.com/stfc/janus-core/raw/main/tests/models/mace_mp_small.model\"\n",
    "model = ModelData.from_uri(uri, architecture=\"mace_mp\", cache_dir=\"mlips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c68186",
   "metadata": {},
   "source": [
    "### Define Initial Structure\n",
    "\n",
    "We create a rocksalt NaCl structure with a lattice parameter of 5.63 Å. This serves as the starting point for our MD simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ba5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida.orm import StructureData\n",
    "from ase.build import bulk\n",
    "from ase.io import read, iread\n",
    "\n",
    "# structure = StructureData(ase=read(\"Structures/qmof-ffeef76.cif\"))\n",
    "init_structure = StructureData(ase=bulk(\"NaCl\", \"rocksalt\", 5.63))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12932e35",
   "metadata": {},
   "source": [
    "### Load Computational Codes\n",
    "\n",
    "We need two codes:\n",
    "- **janus@localhost**: For MLIP calculations (MD, descriptors, training) - runs locally\n",
    "- **qe@scarf**: For DFT calculations with Quantum ESPRESSO - runs on HPC cluster\n",
    "\n",
    "See examples>tutorials>aiida_setup for setting up janus code and external computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida.orm import load_code\n",
    "janus_code = load_code(\"janus@localhost\")\n",
    "qe_code = load_code(\"qe@scarf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240333a",
   "metadata": {},
   "source": [
    "### Define Calculation Types\n",
    "\n",
    "We define three calculation types from the `aiida-mlip` plugin:\n",
    "- `mdCalc`: Molecular dynamics simulation\n",
    "- `descriptorsCalc`: Compute structural descriptors  \n",
    "- `trainCalc`: Fine-tune the MLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4c78c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida.plugins import CalculationFactory\n",
    "\n",
    "mdCalc = CalculationFactory(\"mlip.md\")\n",
    "descriptorsCalc = CalculationFactory(\"mlip.descriptors\")\n",
    "trainCalc = CalculationFactory(\"mlip.train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36acd26d",
   "metadata": {},
   "source": [
    "## Dynamic Task Creation with Descriptors\n",
    "\n",
    "### WorkGraph Concepts\n",
    "\n",
    "WorkGraph provides three task decorators:\n",
    "- `task(CalculationClass)`: Wrap AiiDA calculations\n",
    "- `@task.calcfunction`: Wrap AiiDA calcfunctions  \n",
    "- `@task.graph`: Create a sub-workflow (task graph)\n",
    "\n",
    "The `@task.graph` decorator below with `dynamic(SinglefileData)` outputs creates a pattern: the number of output tasks is determined at runtime based on trajectory length. WorkGraph automatically:\n",
    "1. Tracks dependencies between all generated tasks\n",
    "2. Executes them in parallel when resources allow\n",
    "3. Collects all results into the `final_structs` output\n",
    "\n",
    "### Scientific Purpose\n",
    "\n",
    "After MD generates trajectory snapshots, we calculate descriptors for each structure. These descriptors:\n",
    "- Characterize the local atomic environment around each atom\n",
    "- Enable intelligent filtering of diverse configurations\n",
    "- Are calculated per-element (separate Na and Cl descriptors)\n",
    "- Feed into FPS (Farthest Point Sampling) for optimal train/test split\n",
    "\n",
    "The loop creates one descriptor calculation per MD snapshot, all running in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida_workgraph import WorkGraph, task, dynamic, namespace\n",
    "from aiida.orm import SinglefileData\n",
    "\n",
    "descriptorsTask = task(descriptorsCalc)\n",
    "\n",
    "@task.graph(outputs=namespace(final_structs=dynamic(SinglefileData)))\n",
    "def descriptors_task(\n",
    "    code,\n",
    "    model,\n",
    "    device,\n",
    "    arch,\n",
    "    file,\n",
    "):\n",
    "    results = {}\n",
    "    \n",
    "    with file.as_path() as path:\n",
    "        for i, structs in enumerate(iread(path)):\n",
    "            structure = StructureData(ase=structs)\n",
    "\n",
    "            desc_calc = descriptorsTask(\n",
    "                code=code,\n",
    "                model=model,\n",
    "                device=device,\n",
    "                arch=arch,\n",
    "                struct=structure,\n",
    "                calc_per_element=True,\n",
    "                metadata={\"options\": {\"resources\": {\"num_machines\": 1}}}\n",
    "            )\n",
    "\n",
    "            results[f\"struct{i}\"] = desc_calc.xyz_output\n",
    "\n",
    "    return {\"final_structs\": results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ecc71",
   "metadata": {},
   "source": [
    "## Nested Dynamic Tasks with Quantum ESPRESSO\n",
    "\n",
    "### WorkGraph features:\n",
    "\n",
    "**Namespaces**: `namespace(trajectory=TrajectoryData, parameters=Dict)` defines a structured output with multiple typed fields. Each DFT calculation returns both trajectory (forces/stress) and parameters (energy).\n",
    "\n",
    "**Nested Dynamic Outputs**: The output specification creates a hierarchical structure:\n",
    "- Three fixed categories: `test_file`, `train_file`, `valid_file`\n",
    "- Each category contains a dynamic number of structures (determined at runtime)\n",
    "- Each structure returns a namespace with both trajectory and parameters\n",
    "\n",
    "This task processes three input files and creates dozens of DFT calculations. WorkGraph automatically parallelizes all independent calculations and organizes outputs hierarchically.\n",
    "\n",
    "### Scientific Purpose  \n",
    "\n",
    "Run high-accuracy DFT calculations with Quantum ESPRESSO to obtain reference data:\n",
    "- **Total energy**: Ground state energy for each structure\n",
    "- **Forces**: Atomic forces for training force predictions\n",
    "- **Stress tensor**: Needed for NPT dynamics and equation of state\n",
    "\n",
    "**DFT Settings**:\n",
    "- SCF calculation (self-consistent field)\n",
    "- SSSP pseudopotentials with recommended cutoffs (`ecutwfc`, `ecutrho`)\n",
    "- Γ-point only (1×1×1 k-point mesh) - sufficient for supercell size\n",
    "- `tprnfor=True`: Calculate forces, `tstress=True`: Calculate stress tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de8c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida.orm import (\n",
    "    TrajectoryData,\n",
    "    StructureData, \n",
    "    load_group, \n",
    "    KpointsData, \n",
    "    InstalledCode, \n",
    "    List, \n",
    "    Dict,\n",
    ")\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from aiida_quantumespresso.calculations.pw import PwCalculation\n",
    "\n",
    "PwTask = task(PwCalculation)\n",
    "qe_output = namespace(trajectory=TrajectoryData, parameters=Dict)\n",
    "\n",
    "@task.graph(\n",
    "    outputs=namespace(\n",
    "        test_file=dynamic(qe_output),\n",
    "        train_file=dynamic(qe_output),\n",
    "        valid_file=dynamic(qe_output),\n",
    "    )\n",
    ")\n",
    "def qe(\n",
    "    code: InstalledCode,\n",
    "    kpoints_mesh: List,\n",
    "    task_metadata: Dict,\n",
    "    test_file: SinglefileData,\n",
    "    train_file: SinglefileData,\n",
    "    valid_file: SinglefileData\n",
    "    ):\n",
    "\n",
    "    kpoints = KpointsData()\n",
    "    kpoints.set_kpoints_mesh(kpoints_mesh)\n",
    "\n",
    "    pseudo_family = load_group('SSSP/1.3/PBE/efficiency')\n",
    "    \n",
    "    files = {\"test_file\": test_file, \"train_file\": train_file, \"valid_file\": valid_file}\n",
    "    results = {\"test_file\": {}, \"train_file\": {}, \"valid_file\": {}}\n",
    "    \n",
    "    for file_name, file in files.items():\n",
    "        with file.as_path() as path:\n",
    "            for i, structs in enumerate(iread(path, format=\"extxyz\")):\n",
    "                \n",
    "                structure = StructureData(ase=structs)\n",
    "                pseudos = pseudo_family.get_pseudos(structure=structure)\n",
    "\n",
    "                ecutwfc, ecutrho = pseudo_family.get_recommended_cutoffs(\n",
    "                    structure=structure,\n",
    "                    unit='Ry',\n",
    "                )\n",
    "\n",
    "                pw_params = {\n",
    "                    \"CONTROL\": {\n",
    "                        \"calculation\": \"scf\",\n",
    "                        'tprnfor': True,\n",
    "                        'tstress': True,\n",
    "                    },\n",
    "                    \"SYSTEM\": {\n",
    "                        \"ecutwfc\": ecutwfc,\n",
    "                        \"ecutrho\": ecutrho,\n",
    "                    },\n",
    "                }\n",
    "                \n",
    "                qe_task = PwTask(\n",
    "                    code=code,\n",
    "                    parameters=pw_params,\n",
    "                    kpoints=kpoints,\n",
    "                    pseudos=pseudos,\n",
    "                    metadata=task_metadata.value,\n",
    "                    structure=structure,\n",
    "                )\n",
    "                \n",
    "                structfile = f\"{file_name}.struct{i}\"\n",
    "\n",
    "               \n",
    "                results[file_name][f\"struct_{i}\"] = {\n",
    "                    \"trajectory\": qe_task.output_trajectory,\n",
    "                    \"parameters\": qe_task.output_parameters,\n",
    "                }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcacee8",
   "metadata": {},
   "source": [
    "## CalcFunction Tasks for Data Processing\n",
    "\n",
    "### WorkGraph Concepts\n",
    "\n",
    "The `@task.calcfunction` decorator wraps Python functions with full provenance tracking:\n",
    "- All inputs and outputs are stored in AiiDA's database\n",
    "- Can receive dynamic outputs from upstream tasks (like nested QE results)\n",
    "- Process complex nested data structures\n",
    "- Return AiiDA data types that flow to downstream tasks\n",
    "\n",
    "### Scientific Purpose\n",
    "\n",
    "Extract DFT results from Quantum ESPRESSO and format for MLIP training:\n",
    "\n",
    "1. **Extract data** from QE output:\n",
    "   - Energy from `output_parameters`\n",
    "   - Forces from `output_trajectory`\n",
    "   - Stress tensor from `output_trajectory`\n",
    "\n",
    "2. **Unit conversion**:\n",
    "   - Stress: GPa → eV/Å³ (multiply by `units.GPa`)\n",
    "   - Energy: Already in eV\n",
    "   - Forces: Already in eV/Å\n",
    "\n",
    "3. **Create ExtXYZ files**: Training data format with atomic positions, lattice vectors, and DFT labels (energy, forces, stress) in the info dict and arrays\n",
    "\n",
    "4. **Update config**: Adds file paths to `JanusConfigfile.yml` for MACE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida_mlip.data.config import JanusConfigfile\n",
    "from aiida.orm import Dict\n",
    "from ase.io import write\n",
    "from ase import units\n",
    "\n",
    "@task.calcfunction(outputs = [\"JanusConfigfile\"])\n",
    "def create_train_file(**inputs):\n",
    "\n",
    "    training_files = {}\n",
    "    \n",
    "    for file_name, structs in inputs.items():\n",
    "        path = Path(f\"mlip_{file_name}.extxyz\")\n",
    "\n",
    "        for struct_out_params in structs.values():\n",
    "            \n",
    "            trajectory = struct_out_params[\"trajectory\"]\n",
    "\n",
    "            fileStructure = trajectory.get_structure(index=0)\n",
    "            fileAtoms = fileStructure.get_ase()\n",
    "\n",
    "            stress = trajectory.arrays[\"stress\"][0]\n",
    "            converted_stress = stress * units.GPa\n",
    "            fileAtoms.info[\"qe_stress\"] = converted_stress\n",
    "\n",
    "            fileAtoms.info[\"units\"] = {\n",
    "                \"energy\": \"eV\",\n",
    "                \"forces\": \"ev/Ang\",\n",
    "                \"stress\": \"ev/Ang^3\"\n",
    "            }\n",
    "            fileAtoms.set_array(\"qe_forces\", trajectory.arrays[\"forces\"][0])\n",
    "\n",
    "            parameters = struct_out_params[\"parameters\"]\n",
    "            fileParams = parameters.get_dict()\n",
    "            fileAtoms.info[\"qe_energy\"] = fileParams[\"energy\"]\n",
    "            write(path, fileAtoms, append=True)\n",
    "\n",
    "        training_files[file_name] = str(path.resolve())\n",
    "\n",
    "    with open(\"JanusConfigfile.yml\", \"a\") as f:\n",
    "        yaml.safe_dump(training_files, f, sort_keys=False)\n",
    "\n",
    "    return{'JanusConfigfile': JanusConfigfile(Path(\"JanusConfigfile.yml\").resolve())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5766874",
   "metadata": {},
   "source": [
    "## Data Filtering and Splitting\n",
    "\n",
    "Split the MD trajectory into train/validation/test sets using **FPS (Farthest Point Sampling)** on the descriptor space:\n",
    "- Selects structures that are maximally diverse in configuration space\n",
    "- Ensures training set covers a wide range of atomic environments\n",
    "- Prevents redundant similar structures\n",
    "\n",
    "Note: The warnings about k is too large appear when you have fewer structures than the target split size - the algorithm automatically adjusts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida.orm import Int\n",
    "from sample_split import process_and_split_data\n",
    "\n",
    "@task.calcfunction(outputs = [\"test_file\", \"train_file\", \"valid_file\"])\n",
    "def create_qe_files(**inputs):\n",
    "\n",
    "    n_samples = Int(len(inputs['trajectory_data']))\n",
    "    print(n_samples)\n",
    "\n",
    "    files = process_and_split_data( \n",
    "        **inputs,\n",
    "        n_samples=n_samples,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"train_file\": SinglefileData(files[\"train_file\"]),\n",
    "        \"test_file\": SinglefileData(files[\"test_file\"]),\n",
    "        \"valid_file\": SinglefileData(files[\"valid_file\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff193a2",
   "metadata": {},
   "source": [
    "## Configure Input Parameters\n",
    "\n",
    "Set up parameters for each workflow stage:\n",
    "\n",
    "**MD Simulation**:\n",
    "- **NVT Ensemble**: Constant Number of particles, Volume, and Temperature\n",
    "- Uses thermostat to control temperature, samples structural variations\n",
    "- `steps=10`: Very short MD for tutorial (use 1000+ for real applications)\n",
    "- `traj-every=2`: Save every 2nd snapshot\n",
    "\n",
    "**DFT Calculations**:\n",
    "- Resources: 1 node, 32 MPI processes\n",
    "- K-points: Γ-point only (1×1×1)\n",
    "- Time limit: 1 hour per calculation\n",
    "- HPC-specific: Module loading for Quantum ESPRESSO on SCARF\n",
    "\n",
    "**Data Splitting**:\n",
    "- `scale=1.0e5`: Scaling factor for FPS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida.orm import Str, Float, Bool, Dict\n",
    "inputs = {\n",
    "    \"code\": janus_code,\n",
    "    \"model\": model,\n",
    "    \"arch\": Str(model.architecture),\n",
    "    \"device\": Str(\"cpu\"),\n",
    "    \"metadata\": {\"options\": {\"resources\": {\"num_machines\": 1}}},\n",
    "    \"ensemble\": Str(\"NVT\"),\n",
    "    \"struct\": init_structure,\n",
    "    \"md_kwargs\": Dict(\n",
    "        {\n",
    "            \"steps\": 10,\n",
    "            \"traj-every\": 2\n",
    "        }\n",
    "    )\n",
    "}\n",
    "calc_inputs = {\n",
    "    \"code\": janus_code,\n",
    "    \"model\": model,\n",
    "    \"arch\": Str(model.architecture),\n",
    "    \"device\": Str(\"cpu\"),\n",
    "    \"metadata\": {\"options\": {\"resources\": {\"num_machines\": 1}}},\n",
    "}\n",
    "split_task_inputs = {\n",
    "    \"config_types\": Str(\"\"),\n",
    "    \"prefix\": Str(\"\"),\n",
    "    \"scale\": Float(1.0e5),\n",
    "    \"append_mode\": Bool(False),\n",
    "}\n",
    "qe_inputs = {\n",
    "    \"task_metadata\": Dict({\n",
    "            \"options\": {\n",
    "                \"resources\": {\n",
    "                    \"num_machines\": 1,\n",
    "                    \"num_mpiprocs_per_machine\": 32,\n",
    "                },\n",
    "                \"max_wallclock_seconds\": 3600,\n",
    "                \"queue_name\": \"scarf\",\n",
    "                \"qos\": \"scarf\",\n",
    "                \"environment_variables\": {},\n",
    "                \"withmpi\": True,\n",
    "                \"prepend_text\": \"\"\"\n",
    "                    module purge\n",
    "                    module use /work4/scd/scarf562/eb-common/modules/all\n",
    "                    module load amd-modules\n",
    "                    module load QuantumESPRESSO/7.2-foss-2023a\n",
    "                \"\"\",\n",
    "                \"append_text\": \"\",\n",
    "            },\n",
    "    }),\n",
    "    \"kpoints_mesh\": List([1, 1, 1]),\n",
    "    \"code\": qe_code,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0241edf",
   "metadata": {},
   "source": [
    "## Assembling the WorkGraph\n",
    "\n",
    "Combine all tasks into a single workflow using the `@task.graph()` decorator.\n",
    "\n",
    "### WorkGraph Dependency Management\n",
    "\n",
    "WorkGraph analyzes the data flow and automatically determines execution order:\n",
    "- `md_calc` → `descriptors_calc` (via `traj_file`)\n",
    "- `descriptors_calc` → `split_task` (via `final_structs`)\n",
    "- `split_task` → `qe_task` (via train/test/valid files)\n",
    "- `qe_task` → `create_train_file` (via DFT results)\n",
    "- `create_train_file` → `train_task` (via config file)\n",
    "\n",
    "### Parallel Execution\n",
    "\n",
    "Tasks run in parallel when independent:\n",
    "- All descriptor calculations execute simultaneously (one per MD snapshot)\n",
    "- All QE calculations within each dataset run in parallel\n",
    "- Local tasks (MD, descriptors, training) don't block remote execution (DFT on HPC)\n",
    "\n",
    "You don't manage job submission or dependencies manually - WorkGraph handles it all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdTask = task(mdCalc)\n",
    "trainTask = task(trainCalc)\n",
    "\n",
    "@task.graph()\n",
    "def md_training_workflow(\n",
    "    janus_code,\n",
    "    qe_code,\n",
    "    model,\n",
    "    md_kwargs,\n",
    "    ensemble,\n",
    "    init_structure,\n",
    "    split_inputs,\n",
    "    qe_metadata,\n",
    "    qe_kpoints,\n",
    "):\n",
    "    md_calc = mdTask(\n",
    "        code=janus_code,\n",
    "        model=model,\n",
    "        arch=Str(model.architecture),\n",
    "        device=Str(\"cpu\"),\n",
    "        metadata={\"options\": {\"resources\": {\"num_machines\": 1}}},\n",
    "        ensemble=ensemble,\n",
    "        struct=init_structure,\n",
    "        md_kwargs=md_kwargs\n",
    "    )\n",
    "\n",
    "    descriptors_calc = descriptors_task(\n",
    "        code=janus_code,\n",
    "        model=model,\n",
    "        device=Str(\"cpu\"),\n",
    "        arch=Str(model.architecture),\n",
    "        file=md_calc.traj_file\n",
    "    )\n",
    "   \n",
    "    split_task = create_qe_files(\n",
    "        **split_inputs,\n",
    "        trajectory_data=descriptors_calc.final_structs,\n",
    "    )\n",
    "    \n",
    "    qe_task = qe(\n",
    "        code=qe_code,\n",
    "        kpoints_mesh=qe_kpoints,\n",
    "        task_metadata=qe_metadata,\n",
    "        test_file=split_task.test_file,\n",
    "        train_file=split_task.train_file,\n",
    "        valid_file=split_task.valid_file,\n",
    "    )\n",
    "\n",
    "    training_files = create_train_file(\n",
    "        test_file=qe_task.test_file,\n",
    "        train_file=qe_task.train_file,\n",
    "        valid_file=qe_task.valid_file,\n",
    "    )\n",
    "\n",
    "    train_task = trainTask(\n",
    "        mlip_config=training_files.JanusConfigfile,\n",
    "        code=janus_code,\n",
    "        foundation_model=model,\n",
    "        metadata={\"options\": {\"resources\": {\"num_machines\": 1}}},\n",
    "        fine_tune=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b87291f",
   "metadata": {},
   "source": [
    "### Build the WorkGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3683830",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg = md_training_workflow.build(\n",
    "    janus_code=janus_code,\n",
    "    qe_code=qe_code,\n",
    "    model=model,\n",
    "    md_kwargs=inputs[\"md_kwargs\"],\n",
    "    ensemble=inputs[\"ensemble\"],\n",
    "    init_structure=init_structure,\n",
    "    split_inputs=split_task_inputs,\n",
    "    qe_metadata=qe_inputs[\"task_metadata\"],\n",
    "    qe_kpoints=qe_inputs[\"kpoints_mesh\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9639a",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fd843a",
   "metadata": {},
   "source": [
    "### Run the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ed58e",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "**Scientific**: Fine-tuned MACE model for NaCl with DFT-labeled training data (energies, forces, stresses)\n",
    "\n",
    "**WorkGraph**: Demonstrated dynamic task creation, nested namespaces, automatic dependencies, and parallel execution with full provenance tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b2a6f",
   "metadata": {},
   "source": [
    "## Training Plot\n",
    "\n",
    "If we want to get the training plot, we have to pull it from the remote folder. First lets see whats in there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f757e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the training task's remote folder\n",
    "folder = wg.tasks.Train.outputs.remote_folder.value\n",
    "print(\"Available files and directories in remote folder:\")\n",
    "print(\"-\" * 50)\n",
    "for item in folder.listdir():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c6b28b",
   "metadata": {},
   "source": [
    "Explore the results to find the PNG that the MACE trainer generates and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f5f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the results subdirectory\n",
    "print(\"Files in results directory:\")\n",
    "print(\"-\" * 50)\n",
    "for item in folder.listdir('results'):\n",
    "    print(item)\n",
    "\n",
    "folder.getfile(relpath='results/test_run-123_train_Default_stage_one.png',destpath=picturePath)\n",
    "\n",
    "img = mpimg.imread(picturePath)\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiida-mlip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
